Lab Assignment 3 - Source Code

Task 1:

import matplotlib.pyplot as plot

from sklearn import datasets
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

iris = datasets.load_iris()

data = iris.data
target = iris.target
input_data = iris.target_names

from sklearn.model_selection import train_test_split
X_training_Data, X_testing_Data, y_training_Data, y_testing_Data = train_test_split(data, target, test_size=0.732)

from sklearn.neighbors import KNeighborsClassifier
classifier_input = KNeighborsClassifier(n_neighbors=5)
classifier_input.fit(X_training_Data, y_training_Data)

y_predictor = classifier_input.predict(X_testing_Data)


lda_data = LinearDiscriminantAnalysis(n_components=2)
X_r2 = lda_data.fit(X_testing_Data, y_predictor).transform(data)

plot.figure()
colors_array = ['red', 'green', 'blue']
lw = 2


for color, i, target_name in zip(colors_array, [0, 1, 2], input_data):
    plot.scatter(X_r2[target == i, 0], X_r2[target == i, 1], alpha=.8, color=color,
                label=target_name)
plot.legend(loc='best', shadow=False, scatterpoints=1)
plot.title('Prediction Model of IRIS Data using LDA')

plot.show()


Task 2:

import numpy as np
from sklearn import datasets
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn import metrics

wine_data = datasets.load_wine()
x_data = wine_data.data[:,:2]
y_data = wine_data.target

z=0.3
xaxis_min, xaxis_max = x_data[:, 0].min() - 1, x_data[:, 0].max() + 1
yaxis_min, yaxis_max = x_data[:, 1].min() - 1, x_data[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(xaxis_min, xaxis_max, z),
                     np.arange(yaxis_min, yaxis_max, z))

x_train,x_test,y_train,y_test=train_test_split(x_data,y_data,test_size=0.2)
train_model = svm.SVC()

predictions_linear = train_model.set_params(kernel='linear').fit(x_train, y_train).predict(x_test)

predictions_rbf = train_model.set_params(kernel='rbf').fit(x_train, y_train).predict(x_test)

accuracy_linear = metrics.accuracy_score(y_test,predictions_linear)
accuracy_rbf = metrics.accuracy_score(y_test,predictions_rbf)

print("Accuracy using kernel=linear-",accuracy_linear)
print("Accuracy using kernel=rbf-",accuracy_rbf)

Task 3:

import nltk
from nltk import word_tokenize
from nltk.util import ngrams
from collections import Counter

import re
import urllib.request

def fn_tokens(text):
    return re.findall('[a-z]+', text.lower())

#Reading Text file
with open("inputfile",'r') as f:
    content_input = f.read()

WORDS_input = fn_tokens(open('inputfile').read())
print(WORDS_input)
print(content_input)

#Tokenization
from nltk.stem import WordNetLemmatizer
print("\n")
print("LEMMATIZATION")
lemmatize_list_input = []
lemmatizer=WordNetLemmatizer()
for WORD in WORDS_input:
    print(lemmatizer.lemmatize(WORD))

#Bi-GRAM
from nltk import word_tokenize
from nltk.util import ngrams
print("\n\n")
print("BI-GRAM")
input_list = []
for WORD in WORDS_input:
    input_list=input_list+[WORD]

def fn_find_trigrams(input_list):
  bigram_list_input = []
  for i in range(len(input_list)-2):
      bigram_list_input.append((input_list[i], input_list[i+1]))
  return bigram_list_input
print(fn_find_trigrams(input_list))
print("\n")

#Bi-gram frequency
print("\n")
print("BIGRAM FREQUENCY")
frequencies_counter = Counter([])
bigrams = ngrams(WORDS_input, 2)
frequencies_counter += Counter(bigrams)
print(frequencies_counter)

#Top five bi-gram word
print("\n")
print("TOP FIVE BI-GRAM WORDS")
topFiveBG=list()
for i in range (0,5) :
    topFiveBG.append(" ".join(re.findall("[a-zA-Z]+",str(frequencies_counter).split(":")[i])))
print(topFiveBG)

#Finding all the sentences with those most repeated bi-grams
print("\n")
print("ALL SENTENCE WITH MOST BI-GRAM WORDS")
lines={}
for line in content_input.split("."):
    for word in topFiveBG:
        if word in line:
            if line in lines:
                pass
            else:
                lines[line]=""
result=list()
for line in lines:
    result.append(line+".")
print(result)

Task 4:

from collections import OrderedDict
import csv
import matplotlib.pyplot as plot
from sklearn.cluster import KMeans

i1 = []
s1 = []
def getData(filename):
    with open(filename,'r') as csvdocument:
        csv_File_Content = csv.reader(csvdocument)
        next(csv_File_Content)  # skipping column names
        for row in csv_File_Content:
            i1.append(int(row[3]))
            s1.append(int(row[4]))
    return
getData('StudentDetail.csv')
data = list(zip(i1,s1))
print("Data:")
print(data)

kmeans_data = KMeans(n_clusters=4)
kmeans_data.fit(data)

centroids = kmeans_data.cluster_centers_
labels_data = kmeans_data.labels_
print("Centroids:")
print(centroids)
colors_array = ["m","b","r","y"]
label_array = ["Cluster-1","Cluster-2","Cluster-3","Cluster-3"]

for i1 in range(len(data)):
    print("coordinate:",data[i1], "label_array:", labels_data[i1])
    plot.scatter(data[i1][0], data[i1][1], c = colors_array[labels_data[i1]], label_array = label_array[labels_data[i1]])

plot.scatter(centroids[:, 0],centroids[:, 1], label_array = "Centroids",marker = "x", s1=150, linewidths = 10, zorder = 15)
plot.title('Clusters of Student')
plot.xlabel('SAT Mark')
plot.ylabel('Grade %')
#remove duplicates of labels_data
handles, labels_data = plot.gca().get_legend_handles_labels()
by_label = OrderedDict(zip(labels_data, handles))
plot.legend(by_label.values(), by_label.keys())
plot.show()
#predictions
print("Predicted Class:")
print(kmeans_data.predict([(20,40)]))